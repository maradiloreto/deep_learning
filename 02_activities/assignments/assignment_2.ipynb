{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maradiloreto/deep_learning/blob/assignment-2/02_activities/assignments/assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creative Text Generation with Recurrent Neural Networks (RNNs)\n",
        "\n",
        "In this assignment, you'll build upon your understanding of RNNs and Keras to develop a word-level text generation model.  Your goal is to train a model that learns the stylistic nuances of a chosen corpus and generates new, original text segments that echo the source material's essence.\n",
        "\n",
        "**Datasets**\n",
        "\n",
        "We've provided several intriguing text corpora to get you started:\n",
        "\n",
        "*   Mark Twain\n",
        "*   Charles Dickens\n",
        "*   William Shakespeare\n",
        "\n",
        "**Feel free to explore!**  If you have a particular passion for another author, genre, or a specific text, you're encouraged to use your own dataset of raw text."
      ],
      "metadata": {
        "collapsed": false,
        "id": "7c6788aef474ca12"
      },
      "id": "7c6788aef474ca12"
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Default GPU Device: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "# Check if we have a GPU available\n",
        "import tensorflow as tf\n",
        "if tf.test.gpu_device_name():\n",
        "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
        "else:\n",
        "    print(\"No GPU available. If you're on Colab, go to Runtime > Change runtime and select a GPU hardware accelerator.\")"
      ],
      "metadata": {
        "id": "2d0bfedcfe52aedc",
        "outputId": "6746e246-5c51-448f-a7a8-aac02540740e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2d0bfedcfe52aedc",
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choose a book to download:\n",
            "1: Charles Dickens\n",
            "2: Mark Twain\n",
            "3: William Shakespeare\n",
            "Enter the number corresponding to your choice (1, 2, or 3): 2\n",
            "Downloaded mark_twain.txt successfully!\n",
            "Dataset loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "def download_file(book_name):\n",
        "    base_url = \"https://raw.githubusercontent.com/UofT-DSI/deep_learning/refs/heads/main/02_activities/assignments/downloaded_books/\"\n",
        "    file_url = base_url + book_name\n",
        "    local_filename = book_name\n",
        "\n",
        "    response = requests.get(file_url)\n",
        "    if response.status_code == 200:\n",
        "        with open(local_filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(response.text)\n",
        "        print(f\"Downloaded {book_name} successfully!\")\n",
        "        return local_filename\n",
        "    else:\n",
        "        raise ValueError(\"Failed to download the file. Please check the filename and try again.\")\n",
        "\n",
        "def load_dataset(file_path, fraction=1.0):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        raw_text = f.read()\n",
        "    return raw_text[:int(fraction * len(raw_text))]\n",
        "\n",
        "# Prompt user to select a book\n",
        "title_options = {\n",
        "    \"1\": \"charles_dickens.txt\",\n",
        "    \"2\": \"mark_twain.txt\",\n",
        "    \"3\": \"shakespeare.txt\"\n",
        "}\n",
        "\n",
        "print(\"Choose a book to download:\")\n",
        "print(\"1: Charles Dickens\")\n",
        "print(\"2: Mark Twain\")\n",
        "print(\"3: William Shakespeare\")\n",
        "\n",
        "choice = None\n",
        "while choice not in title_options:\n",
        "    choice = input(\"Enter the number corresponding to your choice (1, 2, or 3): \").strip()\n",
        "    if choice not in title_options:\n",
        "        print(\"Invalid choice. Please enter 1, 2, or 3.\")\n",
        "\n",
        "selected_book = title_options[choice]\n",
        "file_path = download_file(selected_book)\n",
        "\n",
        "# Load chosen dataset\n",
        "fraction = 0.1  # Adjust fraction if running out of memory\n",
        "text = load_dataset(file_path, fraction=fraction)\n",
        "\n",
        "print(\"Dataset loaded successfully!\")"
      ],
      "metadata": {
        "id": "9c28c497f620b775",
        "outputId": "bed09b58-c045-406d-8896-fbd06ec77ce8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "9c28c497f620b775",
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data Preparation (10 Marks)\n",
        "\n",
        "Before we can begin training an RNN model, we need to prepare the dataset. This involves cleaning the text, tokenizing words, and creating sequences the model can be trained on.\n",
        "\n",
        "## 1.1 Data Exploration (3 Marks)\n",
        "\n",
        "Print the first 1000 characters of the dataset. Report the dataset's size and the number of unique characters it contains."
      ],
      "metadata": {
        "collapsed": false,
        "id": "dab51c764031e606"
      },
      "id": "dab51c764031e606"
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution\n",
        "print(text[:1000])\n",
        "print('The dataset contains {} characters.'.format(len(text)))\n",
        "unique_chars = sorted(set(text))\n",
        "print('The dataset contains {} unique characters.'.format(len(unique_chars)))"
      ],
      "metadata": {
        "id": "BunkZmdkl0Wn",
        "outputId": "dd15bbaa-136e-4562-9b90-a192231f2962",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "BunkZmdkl0Wn",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The Project Gutenberg EBook of The Prince and The Pauper, Complete by\n",
            "Mark Twain (Samuel Clemens)\n",
            "\n",
            "This eBook is for the use of anyone anywhere at no cost and with almost\n",
            "no restrictions whatsoever. You may copy it, give it away or re-use\n",
            "it under the terms of the Project Gutenberg License included with this\n",
            "eBook or online at www.gutenberg.org\n",
            "\n",
            "Title: The Prince and The Pauper, Complete\n",
            "\n",
            "Author: Mark Twain (Samuel Clemens)\n",
            "\n",
            "Release Date: August 20, 2006 [EBook #1837]\n",
            "Last Updated: February 19, 2018\n",
            "\n",
            "Language: English\n",
            "\n",
            "Character set encoding: UTF-8\n",
            "\n",
            "*** START OF THIS PROJECT GUTENBERG EBOOK PRINCE AND THE PAUPER ***\n",
            "\n",
            "Produced by David Widger. The earliest PG edition was prepared by Les\n",
            "Bowler\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "THE PRINCE AND THE PAUPER\n",
            "\n",
            "by Mark Twain\n",
            "\n",
            "The Great Seal\n",
            "\n",
            "I will set down a tale as it was told to me by one who had it of his\n",
            "father, which latter had it of HIS father, this last having in like\n",
            "manner had it of HIS father--and so on, back and still back, three\n",
            "hundred years and more, the fat\n",
            "The dataset contains 1267477 characters.\n",
            "The dataset contains 90 unique characters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Text Pre-Processing (4 Marks)\n",
        "\n",
        "To prepare the dataset for training, we need to clean the text and create a numerical representation the model can interpret. Perform the following pre-processing steps:\n",
        "\n",
        "*   Convert the entire text to lowercase.\n",
        "*   Use the `Tokenizer` class from the `keras.preprocessing.text` module to tokenize the text. You should fit the tokenizer on the text and then convert the text to a sequence of numbers. You can use the `texts_to_sequences` method to do this.\n",
        "\n",
        "**Note**:\n",
        "* You'll need to specify an appropriate size for the vocabulary. The number of words in the list of most common words can serve as a guide - does it seem like a reasonable vocabulary size?\n",
        "* Some of the words will be excluded from the vocabulary, as they don't appear often enough. It's important to provide a value for `oov_token` when creating the Tokenizer instance, so that these words can be represented as \"unknown\"."
      ],
      "metadata": {
        "collapsed": false,
        "id": "3ae1639f5ecfe587"
      },
      "id": "3ae1639f5ecfe587"
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# tokenize the text\n",
        "VOCAB_SIZE = 100\n",
        "OOV_TOKEN = \"<OOV>\"\n",
        "\n",
        "# convert the text to lowercase\n",
        "lower_case_text = text.lower()\n",
        "text_lines = lower_case_text.split('\\n')\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=OOV_TOKEN)\n",
        "tokenizer.fit_on_texts(text_lines)"
      ],
      "metadata": {
        "id": "4d0d30cd98ea453c"
      },
      "id": "4d0d30cd98ea453c",
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "source": [
        "If everything worked, the following line should show you the first 10 words in the vocabulary:"
      ],
      "metadata": {
        "collapsed": false,
        "id": "89d32bb9356f711"
      },
      "id": "89d32bb9356f711"
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('<OOV>', 1), ('the', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('in', 7), ('was', 8), ('he', 9), ('it', 10)]\n"
          ]
        }
      ],
      "source": [
        "print(list(tokenizer.word_index.items())[:10])"
      ],
      "metadata": {
        "id": "6a7cd547a19feece",
        "outputId": "e8d3448d-6564-479e-9d96-2458032ac88a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "6a7cd547a19feece",
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Sequence Generation (3 Marks)\n",
        "\n",
        "Now that the text has been tokenized, we need to create sequences the model can be trained on. There are two parts to this:\n",
        "\n",
        "*   Use the `texts_to_sequences` method from the tokenizer to convert the text to a list of sequences of numbers.\n",
        "*   Generate the training sequences. Each training sequence should contain `SEQ_LENGTH` token IDs from the text. The target token for each sequence should be the word that follows the sequence in the text."
      ],
      "metadata": {
        "collapsed": false,
        "id": "da504e4bc6617613"
      },
      "id": "da504e4bc6617613"
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "SEQ_LENGTH = 50  #Choose an appropriate sequence length.\n",
        "\n",
        "# Convert the text to a list of sequences of numbers.\n",
        "sequences = tokenizer.texts_to_sequences(text_lines)\n",
        "\n",
        "flattened_sequence = [token for seq in sequences for token in seq]\n",
        "\n",
        "# Generate the training sequences\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in range(SEQ_LENGTH, len(flattened_sequence)):\n",
        "    X.append(flattened_sequence[i - SEQ_LENGTH:i])\n",
        "    y.append(flattened_sequence[i])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ],
      "metadata": {
        "id": "4ff5fc8d0273709c"
      },
      "id": "4ff5fc8d0273709c",
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assuming your sequences are stored in `X` and the corresponding targets in `y`, the following line should print the first training sequence and its target:"
      ],
      "metadata": {
        "collapsed": false,
        "id": "3b6bdc0deb930df1"
      },
      "id": "3b6bdc0deb930df1"
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence: [ 4  2 84  3  2  1  1 24  1  1  1  1 22  1 20 16  2  1  4  1  1 23 46  1\n",
            "  3 14  1 46  1  1 28  1  1 10  1 10 86 36  1  1 10  1  2  1  4  2  1  1\n",
            "  1  1]\n",
            "Target: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0.]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unhashable type: 'numpy.ndarray'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-dcd257d6fe37>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Sequence: {X[0]}\\nTarget: {y[0]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Translated back to words: {[tokenizer.index_word[i] for i in X[0]]} -> {tokenizer.index_word[y[0]]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
          ]
        }
      ],
      "source": [
        "print(f'Sequence: {X[0]}\\nTarget: {y[0]}')\n",
        "print(f'Translated back to words: {[tokenizer.index_word[i] for i in X[0]]} -> {tokenizer.index_word[y[0]]}')"
      ],
      "metadata": {
        "id": "a495cab04001ce92",
        "outputId": "2309afdc-b8ac-45fe-d05a-b6e7801d832b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "id": "a495cab04001ce92",
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "source": [
        "And the following code will transform y into a one-hot encoded matrix, and split everything into training and validation sets:"
      ],
      "metadata": {
        "collapsed": false,
        "id": "d5bb2c55da17aaa0"
      },
      "id": "d5bb2c55da17aaa0"
    },
    {
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "boolean index did not match indexed array along dimension 1; dimension is 50 but corresponding boolean dimension is 100",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-d17c59d28440>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# One last thing: let's drop any examples where the target is the OOV token - we don't want our model to predict that (boring!)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOOV_TOKEN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 1; dimension is 50 but corresponding boolean dimension is 100"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Convert X and y to numpy arrays\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# One last thing: let's drop any examples where the target is the OOV token - we don't want our model to predict that (boring!)\n",
        "mask = y != tokenizer.word_index[OOV_TOKEN]\n",
        "X = X[mask]\n",
        "y = y[mask]\n",
        "\n",
        "# One-hot encode the target token\n",
        "y = to_categorical(y, num_classes=VOCAB_SIZE)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f'X_train shape: {X_train.shape}')\n",
        "print(f'y_train shape: {y_train.shape}')\n"
      ],
      "metadata": {
        "id": "3a929b2e6c2cc921",
        "outputId": "a55df305-fd51-4a90-b060-a07a2b3be858",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "id": "3a929b2e6c2cc921",
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Model Development (10 Marks)\n",
        "\n",
        "With the dataset prepared, it's time to develop the RNN model. You'll need to define the architecture of the model, compile it, and prepare it for training.\n",
        "\n",
        "## 2.1 Model Architecture (4 Marks)\n",
        "\n",
        "Define the architecture of your RNN model. You can design it however you like, but there are a few features that it's important to include:\n",
        "\n",
        "*   An embedding layer that learns a dense representation of the input tokens. You'll need to specify the input dimension (the size of the vocabulary) and the output dimension (the size of the dense representation). Remember, you can look at the documentation [here](https://keras.io/api/layers/core_layers/embedding/).\n",
        "*   At least one recurrent layer. We have learned how to use LSTM layers in class, but you can use other types of recurrent layers if you prefer. You can find the documentation [here](https://keras.io/api/layers/recurrent_layers/lstm/).\n",
        "*   A dense layer with a softmax activation function. This layer will output a probability distribution over the vocabulary, so that the model can make predictions about the next token."
      ],
      "metadata": {
        "collapsed": false,
        "id": "b6e4161897210434"
      },
      "id": "b6e4161897210434"
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "model = Sequential([\n",
        "\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "9fdfaad93818fc8d"
      },
      "id": "9fdfaad93818fc8d",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Model Compilation (3 Marks)\n",
        "\n",
        "Compile the model with an appropriate loss function and optimizer. You might also want to track additional metrics, such as accuracy.\n",
        "\n",
        "Give a short explanation of your choice of loss function and optimizer:\n",
        "\n",
        "_your explanation here_"
      ],
      "metadata": {
        "collapsed": false,
        "id": "2fafd2dbb0d589fc"
      },
      "id": "2fafd2dbb0d589fc"
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "# Solution\n"
      ],
      "metadata": {
        "id": "ae4ca7a12051b1fd"
      },
      "id": "ae4ca7a12051b1fd",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Model Training (3 Marks)\n",
        "\n",
        "Train the model on the training data you've prepared.\n",
        "\n",
        "* Train your model for 5 epochs with a batch size of 128. Use the validation data for validation.\n",
        "* Store the training history in a variable called `history`."
      ],
      "metadata": {
        "collapsed": false,
        "id": "c2f0b90a448c4f4b"
      },
      "id": "c2f0b90a448c4f4b"
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "# Train the model\n"
      ],
      "metadata": {
        "id": "256b1ea138c67ef7"
      },
      "id": "256b1ea138c67ef7",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the training history to visualize the model's learning progress. Your plot should include the training and validation loss."
      ],
      "metadata": {
        "collapsed": false,
        "id": "195c59bf80d2a2c4"
      },
      "id": "195c59bf80d2a2c4"
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "# Solution\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "9e8cacec70d8f313"
      },
      "id": "9e8cacec70d8f313",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Text Generation (10 Marks)\n",
        "\n",
        "## Task Overview\n",
        "\n",
        "In this task, you will write a function called `generate_text` that uses a trained RNN model to generate new text based on a given seed phrase.\n",
        "\n",
        "Your function **must** follow a structured approach to text generation, where the model predicts one word at a time, adds it to the sequence, and repeats this process until a desired length is reached.\n",
        "\n",
        "## Function Requirements\n",
        "\n",
        "You need to implement a function with the following signature:\n",
        "\n",
        "```python\n",
        "def generate_text(model, tokenizer, seed_text, max_sequence_len, n_words=100):\n",
        "```\n",
        "\n",
        "### **Parameters**\n",
        "- `model`: The trained RNN model that will generate text.\n",
        "- `tokenizer`: The tokenizer used to convert words to numerical sequences.\n",
        "- `seed_text`: The initial text that will be used to start generating words.\n",
        "- `max_sequence_len`: The maximum length of input sequences (same as used in training).\n",
        "- `n_words` (optional, default=100): The number of words to generate.\n",
        "\n",
        "### **Expected Output**\n",
        "- A single **string** containing the generated text.\n",
        "\n",
        "---\n",
        "\n",
        "## **Step-by-Step Instructions**\n",
        "\n",
        "### **1. Tokenize the seed text**\n",
        "Use the tokenizer to convert `seed_text` into a sequence of numbers:\n",
        "\n",
        "```python\n",
        "encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "```\n",
        "\n",
        "### **2. Pad the sequence to match training input length**\n",
        "Ensure that the sequence is the correct length by padding it **at the beginning**:\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "encoded = pad_sequences([encoded], maxlen=max_sequence_len, truncating='pre')\n",
        "```\n",
        "\n",
        "### **3. Predict the next word**\n",
        "Pass the padded sequence to the model to predict the next word.\n",
        "\n",
        "- The model will output a probability distribution over the vocabulary.\n",
        "- Use `np.random.choice` or `np.argmax` to select the most likely word.\n",
        "\n",
        "```python\n",
        "yhat = model.predict(encoded, verbose=0)\n",
        "predicted_word_index = np.argmax(yhat)  # Select the word with the highest probability\n",
        "```\n",
        "\n",
        "### **4. Convert the predicted word index to a word**\n",
        "Find the corresponding word in the tokenizerâ€™s vocabulary:\n",
        "\n",
        "```python\n",
        "out_word = tokenizer.index_word[predicted_word_index]\n",
        "```\n",
        "\n",
        "### **5. Append the new word to the generated text**\n",
        "- Add the predicted word to `seed_text`.\n",
        "- Repeat the process to generate multiple words.\n",
        "\n",
        "```python\n",
        "seed_text += \" \" + out_word\n",
        "```\n",
        "\n",
        "### **6. Repeat Steps 3-5 until `n_words` have been generated**\n",
        "\n",
        "- Each time, remove the oldest word from the input sequence to keep its length constant.\n",
        "- Continue generating words one at a time until reaching `n_words`.\n",
        "\n",
        "---\n",
        "\n",
        "## **Important Notes**\n",
        "- If the generated text doesnâ€™t make much sense, donâ€™t worry! The quality will improve as the model is trained better.\n",
        "- This is a **challenging** task! If you get stuck, ask for help.\n",
        "- The `generate_text` function should return the **full generated text as a single string**.\n",
        "\n",
        "### **Example Usage**\n",
        "After implementing `generate_text`, you should be able to call it like this:\n",
        "\n",
        "```python\n",
        "generated_text = generate_text(model, tokenizer, \"Once upon a time\", max_sequence_len=20, n_words=50)\n",
        "print(generated_text)\n",
        "```\n",
        "\n",
        "This should output a string of 50 words generated by the model, starting with `\"Once upon a time\"`."
      ],
      "metadata": {
        "collapsed": false,
        "id": "3d9ed71305787aed"
      },
      "id": "3d9ed71305787aed"
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "# Solution\n"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-02-08T21:38:43.251561Z",
          "start_time": "2024-02-08T21:38:20.349248Z"
        },
        "id": "d73dbf278a1265ef"
      },
      "id": "d73dbf278a1265ef",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "# Test the text generation function\n",
        "generate_text(model, tokenizer, 'hamlet', SEQ_LENGTH)"
      ],
      "metadata": {
        "id": "f463b0c3df49e2c"
      },
      "id": "f463b0c3df49e2c",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Model Refinement (5 Marks)\n",
        "\n",
        "## **Understanding Your Model's Performance**\n",
        "\n",
        "At this stage, you might have noticed that the text generated by your model doesnâ€™t make much sense yet. **This is completely expected!**\n",
        "\n",
        "There are a few reasons why:\n",
        "1. **RNNs have limitations** â€“ While they can generate sequences, they struggle with long-range dependencies in text.\n",
        "2. **Character-by-character generation is outdated** â€“ Modern models like ChatGPT donâ€™t generate text one letter at a time. Instead, they use **tokens**, which represent larger chunks of words, making their outputs much more coherent.\n",
        "3. **Training time and data size** â€“ Our model has been trained on a relatively small dataset for a short period of time, which means it hasnâ€™t learned enough patterns to generate meaningful text.\n",
        "\n",
        "Even though we donâ€™t expect ChatGPT-level performance, this exercise is about **experimentation, not perfection**. Your goal here is to try **at least one** way to refine your model and observe how it affects the output.\n",
        "\n",
        "---\n",
        "\n",
        "## **Refining Your Model**\n",
        "There are many ways to try improving your model. Here are some ideas:\n",
        "\n",
        "âœ… **Use pre-trained embeddings**  \n",
        "   Instead of learning word representations from scratch, you can use pre-trained word embeddings. This allows your model to start with a better understanding of word relationships.\n",
        "\n",
        "âœ… **Modify the model architecture**  \n",
        "   - Experiment with **more layers** or different numbers of units per layer.  \n",
        "   - Try adding **dropout layers** to prevent overfitting.  \n",
        "   - Consider using **bidirectional RNNs**, which process text in both forward and backward directions.  \n",
        "\n",
        "âœ… **Train for longer**  \n",
        "   - Try increasing the number of **epochs** (but be mindful of overfitting).  \n",
        "   - Experiment with different **batch sizes** to see if they affect training stability.  \n",
        "\n",
        "Again, **perfection is NOT the goal here** â€“ we just want to see that you experimented with improving your model! ðŸš€"
      ],
      "metadata": {
        "collapsed": false,
        "id": "5871d836a0135c41"
      },
      "id": "5871d836a0135c41"
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "!wget https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "id": "dda8b0f845c20862"
      },
      "id": "dda8b0f845c20862",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "# Load the pre-trained embeddings\n",
        "embeddings_index = {}\n",
        "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(f'Found {len(embeddings_index)} word vectors.')"
      ],
      "metadata": {
        "id": "e8b777220505635"
      },
      "id": "e8b777220505635",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "# Create an embedding matrix\n",
        "embedding_matrix = np.zeros((VOCAB_SIZE, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i < VOCAB_SIZE:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "d3e48ff004757cf2"
      },
      "id": "d3e48ff004757cf2",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "embedding_layer = Embedding(\n",
        "    VOCAB_SIZE, 100, weights=[embedding_matrix], trainable=False\n",
        ")"
      ],
      "metadata": {
        "id": "e3d21d5dbbbcf9f9"
      },
      "id": "e3d21d5dbbbcf9f9",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "# Solution\n"
      ],
      "metadata": {
        "id": "f16570310f0f56b"
      },
      "id": "f16570310f0f56b",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "# Test the text generation function\n",
        "generate_text(model, tokenizer, 'hamlet', SEQ_LENGTH)"
      ],
      "metadata": {
        "id": "ae362e2dd29be2e1"
      },
      "id": "ae362e2dd29be2e1",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [],
      "metadata": {
        "id": "236cb723e4e5b3fc"
      },
      "id": "236cb723e4e5b3fc",
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}